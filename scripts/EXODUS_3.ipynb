{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb9c0ee",
   "metadata": {},
   "source": [
    "# Chapter 9:\n",
    "# How to develop LSTMs for Time Series Forecasting\n",
    "\n",
    "After this tutorial, you will know:\n",
    "\n",
    "* How to develop LSTM models fir univariate time series forecasting\n",
    "* How to develop LSTM models fro multivariate time series forecasting\n",
    "* How to develop LSTM models for multi-step time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ad7c0",
   "metadata": {},
   "source": [
    "##  9.2 Univariate LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d4b55",
   "metadata": {},
   "source": [
    "This section is divided into six parts, they are:\n",
    "\n",
    "1. Data Preparation\n",
    "2. Vanilla LSTM\n",
    "3. Stacked LSTM\n",
    "4. Bidirectional LSTM\n",
    "5. CNN-LSTM\n",
    "6. ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e6163",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7e89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    "\n",
    "#split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i +n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        #gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83554181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input sequence\n",
    "raw_seq = array([i for i in range(10, 100, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6805cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021654f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i],y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3f9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timestepsm features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d78a3",
   "metadata": {},
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad211d67",
   "metadata": {},
   "source": [
    "A Vanilla LSTm is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction. Key to LSTMs is that they offer native support for sequences. UNlike a CNN that reads across the entire input vector, the LSTM model reads one time step of the sequence at a time and builds up an internal state representation that can be used as a learned context for making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a4d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67855e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape = (n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a444ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0dfed5760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=2000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4044ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrare prediction\n",
    "x_input = array([70,80,90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da73eae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.45489]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3009d3",
   "metadata": {},
   "source": [
    "### Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eda542",
   "metadata": {},
   "source": [
    "Multiple hidden LSTM layer can be stacked one on top another in what is referred to as a Stacked LSTM Model. An LSTM layer requieres a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence. We can address this by having the LSTM output a value for each time step in the input data by setting the `return_sequences=True` argument on the layer. This allow us to have 3D output from hidden LSTM layer as input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d090069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/think/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define model \n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1200773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0de6d6510>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=2000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3407b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrare prediction\n",
    "x_input = array([70,80,90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23018cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1573ec",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03812e6d",
   "metadata": {},
   "source": [
    "On some sequence prediction probelsm, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations. This is called a Bidirectional LSTM. We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e40093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373b3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/think/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e168a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0b8264c80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=2000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39154871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrare prediction\n",
    "x_input = array([70,80,90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f230917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.3858]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13e4c9",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86d216",
   "metadata": {},
   "source": [
    "A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data. The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data. A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. THis hybrid model si called CNN-LSTM.\n",
    "\n",
    "The first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input/output samples with four steps as input and one as output. EAch sample can be then split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subseqeunces to the LSTM model to process as input. We can parametrize this and define the numbe of subsequences as `n_seq` and the number of time steps per subsequence as `n_steps`. The unput data can then be reshape to have the required structure: `[samples, subsequences, timesteps, features]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11073855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split inot samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7544ab",
   "metadata": {},
   "source": [
    "We want to reuse the same CNN model when reading in each sub-sequence of data separately.\n",
    "This can be achieved bu wrapping the entire CNN model in a `TimeDistributed` wrapper that will apply the entire model once per input, in this case, once per input subsequence. The CNN models first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. The number of filters is the number of reads or interpretations of the input sequence. The kernel size is the number of time steps included of each rad operation of the input seqeunce, The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/4 of their size that includes the most salient featuees. These structures are then flattend down to a single one-dimensional vector to be used as a single input time step to the LSTM layer.\n",
    "\n",
    "```python\n",
    "# define the input cnn model\n",
    "model.add(TimeDistributed(Conv1D(64, 1, activation = 'relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "model.add(TImeDistributed(Flatten))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1e9ba",
   "metadata": {},
   "source": [
    "Next, we can define the LSTM part of the model that interprets the CNN model's read of the input sequence and makes a prediction\n",
    "\n",
    "```python\n",
    "# define the output model\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3490e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10319226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "# define the input cnn model\n",
    "model.add(TimeDistributed(Conv1D(64, 1, activation = 'relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# define the output model\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39d4d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0b8b0f7a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "635501c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrare prediction\n",
    "x_input = array([60,70,80,90])\n",
    "x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "164ecd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.841805]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c93f84",
   "metadata": {},
   "source": [
    "### ConvLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6179e",
   "metadata": {},
   "source": [
    "A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading od input is built directly into each LSTM unit. The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting. The layer expects input as a sequence of two-dimensionalimages, therefore the shape of input data must be: `[samples, timesteps, rows, columns, features]`.\n",
    "\n",
    "\n",
    "For our purposed, we can split each sample into subsequences where timesteps will become the number of subsequences, or `n_seq`, and colunmns will be the number of time steps for each subseqeunce, or `n_steps`. The number of rows is fixed at 1 as we are working with one-dimensional data. We can now reshape the prepared samples into the required structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5c5b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "x, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\n",
    "n_features = 1\n",
    "n_seq =2\n",
    "n_steps = 2 \n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b8155",
   "metadata": {},
   "source": [
    "We can define the ConvLSTM as a single layer in terms of the number of filters and a two-dimensional kernel size in terms of `(rows, columns)`. As we are working with a one-dimensional series, the number of rows is always fixed to 1 in the kernel. THe output of the model must then be flattened before it can be interpreted and a prediction made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c3e5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "680e9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/think/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "# define the input cnnlstm model\n",
    "model.add(ConvLSTM2D(64, (1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0af97cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0de6d77a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X,y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7110826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrare prediction\n",
    "x_input = array([60,70,80,90])\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d18564f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.48329]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924d281",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2319009",
   "metadata": {},
   "source": [
    "## 9.3 Multivariate LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80960a",
   "metadata": {},
   "source": [
    "Multivariate time series data means data where there is more than one observation for each time step. There are two main models that we may require with multivariate time series data; they are:\n",
    "\n",
    "1. Multiple Input Series\n",
    "2. Multiple Parallel Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c7707",
   "metadata": {},
   "source": [
    "### Multiple Input Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff278c0",
   "metadata": {},
   "source": [
    "A problem may have two or more parallel input time series and an output time series that is dependent on the input time series. The input time series are parallel beacuse each series has an observation at the same time steps. We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f080b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "# define input sequence\n",
    "in_seq1 = array([i for i in range(10, 100, 10)])\n",
    "in_seq2 = array([i for i in range(15, 100, 10)])\n",
    "out_seq = array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed9b42",
   "metadata": {},
   "source": [
    "We can reshape these three arrays of data as a single dataset where each row is a time step, and each column is a separate time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c021c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "# convert to [row, columns] structure\n",
    "\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "#horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87484abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b58c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y =  sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48e87dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X,  y = split_sequences(dataset, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3200b",
   "metadata": {},
   "source": [
    "As with the univariate tiem series, we must structure these data into samples with input and output elements. An LSTM model needs sufficient context to learn a mapping from an input sequence to an output value. LSTMs can support parallel input time series as separate variables or features. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences. If we chose three input time steps, then the first sample would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d19d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] 65\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] 85\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] 105\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] 125\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] 145\n",
      "[[60 65]\n",
      " [70 75]\n",
      " [80 85]] 165\n",
      "[[70 75]\n",
      " [80 85]\n",
      " [90 95]] 185\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054dab24",
   "metadata": {},
   "source": [
    "That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case, 65. \n",
    "We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in th einput time series at prior time steps. IN turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee6a7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3, 2)\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21035959",
   "metadata": {},
   "source": [
    "We can see that the X component is three-dimensional. THe first dimension is the number of samples, in thsi case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time serie or the number of variables, in this case 2 for the two parallel series.\n",
    "\n",
    "This is the exact three-dimensional structure expected by an LSTM as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03604dec",
   "metadata": {},
   "source": [
    "W ea re now ready to fit an LSTM model on this data. We will use a Vanilla LSTM where the number of time steps and parallel series (features) are specified for the input layer via the `input_shape` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee446674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/think/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "918ba4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0b90c6180>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs =200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5db49a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = array([[80,85], [90,95], [100,105]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a576bee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d116b269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 80,  85],\n",
       "       [ 90,  95],\n",
       "       [100, 105]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89683917",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7cebd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x76f0b050e8e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([[80,85], [90,95], [100,105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6266ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[207.4449]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf348",
   "metadata": {},
   "source": [
    "### Multiple Parallel Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053684a",
   "metadata": {},
   "source": [
    "An alternate time series problem is the case where there are multiple parallel time series and a avalue must be predicted for each.\n",
    "\n",
    "We may want to predict the value for each of the three time series for the next time step. This might be referred to as multivariate forecasting. Again, the data must be split inot input/output samples in order to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f404cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y =  sequences[i:end_ix, :], sequences[end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2baba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "# define input sequence\n",
    "in_seq1 = array([i for i in range(10, 100, 10)])\n",
    "in_seq2 = array([i for i in range(15, 100, 10)])\n",
    "out_seq = array([in_seq1[i] + in_seq2[i] for i in range(len(in_seq1))])\n",
    "\n",
    "from numpy import hstack\n",
    "# convert to [row, columns] structure\n",
    "\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "#horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16354f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "057586d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 3) (6, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e7c20",
   "metadata": {},
   "source": [
    "The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3). The sahpe of y is two-dimensional as we might expect for the number of samples (6) and the number of variables per sample to be predicted (3).\n",
    "\n",
    "The data is ready to use in an LSTM model that expects three-dimensional input and two-dimensional output shapes for the X and y components of each samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c677534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 15 25]\n",
      " [20 25 45]\n",
      " [30 35 65]] [40 45 85]\n",
      "[[20 25 45]\n",
      " [30 35 65]\n",
      " [40 45 85]] [ 50  55 105]\n",
      "[[ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]] [ 60  65 125]\n",
      "[[ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]] [ 70  75 145]\n",
      "[[ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]] [ 80  85 165]\n",
      "[[ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]] [ 90  95 185]\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050654b",
   "metadata": {},
   "source": [
    "We are now ready to fit an LSTM model on this data. We will use a Stacked LSTM where the number of time steps and parallel series (features) are specified for the input layer via the `input_shape` argument. The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2cb808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60329fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/think/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define model \n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a7374f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0b8f0f7a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs = 400, verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3648f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x76f06868d8a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([[70,75,145], [80,85,165],[90,95,185]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "314884fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.24531  105.234764 205.46284 ]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee485800",
   "metadata": {},
   "source": [
    "## 9.4 Multi-step LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ad3cc",
   "metadata": {},
   "source": [
    "A time series forecasting probelm that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting. Specifically, these are problems where teh forecast horizon or interval is more than one time step. There are two main types of LSTM models that can be used for multi-step forecasting; they are:\n",
    "\n",
    "1. Vector Ouput Model\n",
    "2. Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3201d7",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a1ab2",
   "metadata": {},
   "source": [
    "As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7589868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this patter\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9eadc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input sequence\n",
    "raw_seq = array([i for i in range(10, 100, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38c2ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps_in = 3\n",
    "n_steps_out = 2\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "221acb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] [40 50]\n",
      "[20 30 40] [50 60]\n",
      "[30 40 50] [60 70]\n",
      "[40 50 60] [70 80]\n",
      "[50 60 70] [80 90]\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b43bc3",
   "metadata": {},
   "source": [
    "Now that we know how to prepare data for multi-step forecasting, let's look at some LSTM models that can learn this mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9c5de",
   "metadata": {},
   "source": [
    "### Vector Output Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf1be4",
   "metadata": {},
   "source": [
    "Like other types of nerual network models, the LSTM can output a vector directyl that can be interpreted directly as a multi-step forecast. THis approach was seen in the previous section were one time step of each output time serieas was forecasted as a vector. As with the LSTMs for univariate data in a prior section, the prepared smaples must first be reshaped. The LSTM expects data to have a three-dimensional strcuture of `[samples, timesteps, features]`, and in this case, we only have one feature so the reshape is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b63edadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecdeb0",
   "metadata": {},
   "source": [
    "With the number of onput and outpu steps specified in the `n_steps_in` and `n_steps_out` variables, we can define a multi-step time-series ofrecasting model. Below defines a Stacked LSTM for multi-step forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4c5fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/think/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define mode\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation = 'relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58e61440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76f0b90c73b0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs = 50,verbose= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66835782",
   "metadata": {},
   "source": [
    "As expected by the model, the shape of the single sample of input data when making the prediction must be `[1,3,1]` for the 1 sample, 3 time steps of the input, and the single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8138bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([70,80,90])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b67665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104.6682  122.07481]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c600b",
   "metadata": {},
   "source": [
    "### Encode-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8f193",
   "metadata": {},
   "source": [
    "A model specifically developed for forecasting variable lenght output sequences is called the Encoder-Decoder LSTM. The model was designed for predictio problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating test from one language to another. This model can be used for multi-step time series forecasting. As it name suggests, the model is comprised of two sub-models: the encoder and the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aec873",
   "metadata": {},
   "source": [
    "The encoder is a model responsible for reading and interpreting the input sequence. The ouput od the encoder is a fixed lenght vector that represents the model's interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f7ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
